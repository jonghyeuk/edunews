# Firebase Functions - ì…ì‹œë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œìŠ¤í…œ
import functions_framework
from firebase_admin import initialize_app, firestore
import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime, timedelta
import logging

# Firebase Admin ì´ˆê¸°í™”
initialize_app()

# êµìœ¡ì²­ í¬ë¡¤ë§ ì„¤ì •
EDUCATION_OFFICES = {
    "êµìœ¡ë¶€": {
        "url": "https://www.moe.go.kr/boardCnts/list.do?boardID=294",
        "selector": ".board-list-table tbody tr",
        "title_selector": ".title a",
        "date_selector": ".date"
    },
    "ì„œìš¸ì‹œêµìœ¡ì²­": {
        "url": "https://www.sen.go.kr/web/services/bbs/bbsList.action?bbsBean.bbsCd=140", 
        "selector": ".bbs-list tbody tr",
        "title_selector": ".subject a",
        "date_selector": ".reg-date"
    },
    "ê²½ê¸°ë„êµìœ¡ì²­": {
        "url": "https://www.goe.go.kr/home/bbs/bbsList.do?menuNo=1020408",
        "selector": ".board_list tbody tr",
        "title_selector": ".title a", 
        "date_selector": ".date"
    },
    "ì¸ì²œì‹œêµìœ¡ì²­": {
        "url": "https://www.ice.go.kr/contents.do?menuNo=200047",
        "selector": ".list tbody tr",
        "title_selector": ".subject a",
        "date_selector": ".date"
    }
}

@functions_framework.cloud_event
def weekly_news_crawler(cloud_event):
    """ë§¤ì£¼ ì‹¤í–‰ë˜ëŠ” ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜"""
    logging.info("ğŸš€ ì£¼ê°„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
    
    try:
        # Firestore í´ë¼ì´ì–¸íŠ¸
        db = firestore.client()
        
        all_articles = []
        
        # ê° êµìœ¡ì²­ í¬ë¡¤ë§
        for office_name, config in EDUCATION_OFFICES.items():
            try:
                articles = crawl_education_office(office_name, config)
                all_articles.extend(articles)
                logging.info(f"âœ… {office_name}: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
            except Exception as e:
                logging.error(f"âŒ {office_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
                continue
        
        # ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì„ Firestoreì— ì €ì¥
        if all_articles:
            save_articles_to_firestore(db, all_articles)
            logging.info(f"ğŸ’¾ ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ")
        
        return {
            "status": "success",
            "articles_count": len(all_articles),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logging.error(f"âŒ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}")
        return {
            "status": "error", 
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

def crawl_education_office(office_name, config):
    """ê°œë³„ êµìœ¡ì²­ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
    articles = []
    
    try:
        # í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        
        # ì›¹í˜ì´ì§€ ìš”ì²­
        response = requests.get(config["url"], headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ
        article_elements = soup.select(config["selector"])
        
        for element in article_elements[:5]:  # ìµœì‹  5ê°œë§Œ
            try:
                # ì œëª© ì¶”ì¶œ
                title_element = element.select_one(config["title_selector"])
                if not title_element:
                    continue
                    
                title = title_element.get_text().strip()
                
                # ë§í¬ ì¶”ì¶œ
                link = title_element.get('href', '')
                if link and not link.startswith('http'):
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    base_url = '/'.join(config["url"].split('/')[:3])
                    link = base_url + link
                
                # ë‚ ì§œ ì¶”ì¶œ
                date_element = element.select_one(config["date_selector"])
                date_text = date_element.get_text().strip() if date_element else ''
                
                # ìµœê·¼ 30ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ í•„í„°ë§
                if is_recent_article(date_text):
                    article = {
                        'title': title,
                        'source': office_name,
                        'link': link,
                        'date': date_text,
                        'content': extract_content_summary(title),
                        'category': classify_article(title),
                        'created_at': firestore.SERVER_TIMESTAMP
                    }
                    articles.append(article)
                    
            except Exception as e:
                logging.warning(f"ê¸°ì‚¬ ì¶”ì¶œ ì˜¤ë¥˜ ({office_name}): {str(e)}")
                continue
                
    except Exception as e:
        logging.error(f"{office_name} í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        
    return articles

def is_recent_article(date_text):
    """ìµœê·¼ ê¸°ì‚¬ì¸ì§€ í™•ì¸ (30ì¼ ì´ë‚´)"""
    try:
        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
        date_text = re.sub(r'[^\d\-\.]', '', date_text)
        
        # ë‚ ì§œ íŒŒì‹± ì‹œë„
        for fmt in ['%Y-%m-%d', '%Y.%m.%d', '%m-%d', '%m.%d']:
            try:
                if len(date_text.split('-')) == 2 or len(date_text.split('.')) == 2:
                    date_text = f"{datetime.now().year}-{date_text}"
                
                article_date = datetime.strptime(date_text, fmt)
                thirty_days_ago = datetime.now() - timedelta(days=30)
                return article_date >= thirty_days_ago
            except:
                continue
        
        # íŒŒì‹± ì‹¤íŒ¨ì‹œ ìµœì‹ ìœ¼ë¡œ ê°„ì£¼
        return True
        
    except:
        return True

def extract_content_summary(title):
    """ì œëª©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‚´ìš© ìš”ì•½ ìƒì„±"""
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ìš”ì•½ (ì‹¤ì œë¡œëŠ” ë³¸ë¬¸ í¬ë¡¤ë§ + AI ìš”ì•½)
    keywords = {
        'ì…ì‹œ': 'ëŒ€í•™ì…ì‹œ ê´€ë ¨ ì¤‘ìš” ê³µì§€ì‚¬í•­ì…ë‹ˆë‹¤.',
        'ìˆ˜ëŠ¥': 'ëŒ€í•™ìˆ˜í•™ëŠ¥ë ¥ì‹œí—˜ ê´€ë ¨ ì•ˆë‚´ì‚¬í•­ì…ë‹ˆë‹¤.',
        'ì „í˜•': 'ëŒ€í•™ ì…í•™ì „í˜• ë³€ê²½ ë˜ëŠ” ì•ˆë‚´ì‚¬í•­ì…ë‹ˆë‹¤.',
        'ëª¨ì§‘': 'ëŒ€í•™ ì‹ ì…ìƒ ëª¨ì§‘ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤.',
        'ì›ì„œ': 'ì…í•™ì›ì„œ ì ‘ìˆ˜ ê´€ë ¨ ì•ˆë‚´ì…ë‹ˆë‹¤.',
        'ë©´ì ‘': 'ì…í•™ ë©´ì ‘ì „í˜• ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤.',
        'íŠ¹ëª©ê³ ': 'íŠ¹ëª©ê³  ì…í•™ ê´€ë ¨ ì•ˆë‚´ì‚¬í•­ì…ë‹ˆë‹¤.',
        'ìì‚¬ê³ ': 'ììœ¨í˜•ì‚¬ë¦½ê³  ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤.'
    }
    
    for keyword, summary in keywords.items():
        if keyword in title:
            return summary
    
    return 'ì…ì‹œ ê´€ë ¨ ì¤‘ìš” ì •ë³´ë¥¼ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.'

def classify_article(title):
    """ê¸°ì‚¬ ë¶„ë¥˜"""
    title_lower = title.lower()
    
    if any(word in title_lower for word in ['ëŒ€ì…', 'ìˆ˜ëŠ¥', 'ì…ì‹œ', 'ì „í˜•']):
        return 'major_news'
    elif any(word in title_lower for word in ['ëŒ€í•™êµ', 'ëŒ€í•™']):
        return 'university_news'  
    elif any(word in title_lower for word in ['ì¼ì •', 'ëª¨ì§‘', 'ì ‘ìˆ˜', 'ë§ˆê°']):
        return 'exam_schedule'
    else:
        return 'education_office_news'

def save_articles_to_firestore(db, articles):
    """Firestoreì— ê¸°ì‚¬ ì €ì¥"""
    batch = db.batch()
    
    for article in articles:
        # ì¤‘ë³µ ì²´í¬ (ì œëª© + ì¶œì²˜ ê¸°ì¤€)
        existing = db.collection('news')\
            .where('title', '==', article['title'])\
            .where('source', '==', article['source'])\
            .limit(1)\
            .get()
        
        if not existing:
            # ìƒˆ ê¸°ì‚¬ë§Œ ì €ì¥
            doc_ref = db.collection('news').document()
            batch.set(doc_ref, article)
    
    batch.commit()

@functions_framework.https
def get_latest_news(request):
    """API: ìµœì‹  ë‰´ìŠ¤ ì¡°íšŒ"""
    
    # CORS í—¤ë” ì„¤ì •
    headers = {
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
        'Access-Control-Allow-Headers': 'Content-Type'
    }
    
    if request.method == 'OPTIONS':
        return ('', 204, headers)
    
    try:
        db = firestore.client()
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ì¡°íšŒ
        news_data = {
            'major_news': [],
            'education_office_news': [],
            'university_news': [],
            'exam_schedule': []
        }
        
        # ìµœê·¼ 7ì¼ ë‰´ìŠ¤
        week_ago = datetime.now() - timedelta(days=7)
        
        for category in news_data.keys():
            docs = db.collection('news')\
                .where('category', '==', category)\
                .order_by('created_at', direction=firestore.Query.DESCENDING)\
                .limit(5)\
                .stream()
            
            for doc in docs:
                news_data[category].append(doc.to_dict())
        
        return (json.dumps(news_data, ensure_ascii=False), 200, headers)
        
    except Exception as e:
        logging.error(f"ë‰´ìŠ¤ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return (json.dumps({'error': str(e)}, ensure_ascii=False), 500, headers)

@functions_framework.https  
def manual_crawl(request):
    """ìˆ˜ë™ í¬ë¡¤ë§ íŠ¸ë¦¬ê±° (í…ŒìŠ¤íŠ¸ìš©)"""
    
    headers = {
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
        'Access-Control-Allow-Headers': 'Content-Type'
    }
    
    if request.method == 'OPTIONS':
        return ('', 204, headers)
    
    try:
        # ê°€ì§œ í´ë¼ìš°ë“œ ì´ë²¤íŠ¸ ìƒì„±í•˜ì—¬ í¬ë¡¤ë§ ì‹¤í–‰
        fake_event = type('CloudEvent', (), {})()
        result = weekly_news_crawler(fake_event)
        
        return (json.dumps(result, ensure_ascii=False), 200, headers)
        
    except Exception as e:
        logging.error(f"ìˆ˜ë™ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        return (json.dumps({'error': str(e)}, ensure_ascii=False), 500, headers)
